{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ TweetGuard AI Detector - Model Training\n",
    "\n",
    "This notebook trains a DistilBERT model to detect AI-generated text, optimized for Twitter/X content.\n",
    "\n",
    "## Overview\n",
    "- **Model**: DistilBERT (40% smaller, 60% faster than BERT)\n",
    "- **Task**: Binary classification (Human vs AI-generated)\n",
    "- **Datasets**: HC3, ChatGPT Detection Corpus\n",
    "- **Output**: TensorFlow.js model (~1-2MB)\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with GPU runtime (free tier works)\n",
    "- ~2-4 hours training time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "First, let's install the required packages and set up GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate tensorflowjs torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Datasets\n",
    "\n",
    "We'll use multiple datasets for robust training:\n",
    "1. **HC3** - Human ChatGPT Comparison corpus\n",
    "2. **ChatGPT Detection Corpus** - Additional AI-generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading datasets...\")\n\n# Load HC3 dataset (using the updated method)\ntry:\n    # Try loading with trust_remote_code for newer datasets library\n    hc3 = load_dataset(\"Hello-SimpleAI/HC3\", \"all\", trust_remote_code=True)\n    print(f\"‚úÖ HC3 loaded: {len(hc3['train'])} samples\")\nexcept Exception as e1:\n    try:\n        # Alternative: Load without config name\n        hc3 = load_dataset(\"Hello-SimpleAI/HC3\", trust_remote_code=True)\n        print(f\"‚úÖ HC3 loaded (alt method)\")\n    except Exception as e2:\n        try:\n            # Fallback: Use a different AI detection dataset\n            print(\"‚ö†Ô∏è HC3 not available, trying alternative dataset...\")\n            hc3 = load_dataset(\"artem9k/ai-text-detection-pile\", trust_remote_code=True)\n            print(f\"‚úÖ Alternative dataset loaded\")\n        except Exception as e3:\n            print(f\"‚ùå Could not load datasets: {e3}\")\n            hc3 = None\n\n# If HC3 failed, try another popular AI detection dataset\nif hc3 is None:\n    try:\n        print(\"Trying OpenAI detector dataset...\")\n        hc3 = load_dataset(\"aadityaubhat/GPT-wiki-intro\", trust_remote_code=True)\n        print(f\"‚úÖ GPT-wiki-intro loaded\")\n    except:\n        print(\"‚ùå No datasets available. Please check your internet connection.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_hc3_data(dataset):\n    \"\"\"\n    Prepare HC3 dataset for training.\n    Handles multiple dataset formats.\n    \"\"\"\n    texts = []\n    labels = []\n    \n    # Check the structure of the dataset\n    if hasattr(dataset, 'features'):\n        print(f\"Dataset features: {dataset.features}\")\n    \n    for item in dataset:\n        # HC3 format: 'human_answers' and 'chatgpt_answers'\n        if 'human_answers' in item and item['human_answers']:\n            for answer in item['human_answers']:\n                if answer and len(str(answer).strip()) > 20:\n                    texts.append(str(answer).strip())\n                    labels.append(0)  # Human\n        \n        if 'chatgpt_answers' in item and item['chatgpt_answers']:\n            for answer in item['chatgpt_answers']:\n                if answer and len(str(answer).strip()) > 20:\n                    texts.append(str(answer).strip())\n                    labels.append(1)  # AI\n        \n        # Alternative format: 'text' and 'label'\n        if 'text' in item and 'label' in item:\n            text = str(item['text']).strip()\n            if len(text) > 20:\n                texts.append(text)\n                labels.append(int(item['label']))\n        \n        # GPT-wiki-intro format: 'wiki_intro' (human) and 'generated_intro' (AI)\n        if 'wiki_intro' in item and item['wiki_intro']:\n            text = str(item['wiki_intro']).strip()\n            if len(text) > 20:\n                texts.append(text)\n                labels.append(0)  # Human\n        \n        if 'generated_intro' in item and item['generated_intro']:\n            text = str(item['generated_intro']).strip()\n            if len(text) > 20:\n                texts.append(text)\n                labels.append(1)  # AI\n    \n    return texts, labels\n\n# Process data\nif hc3:\n    # Get the training split (handle different dataset structures)\n    if 'train' in hc3:\n        data_split = hc3['train']\n    else:\n        # If no train split, use the first available split\n        data_split = hc3[list(hc3.keys())[0]]\n    \n    texts, labels = prepare_hc3_data(data_split)\n    print(f\"\\nProcessed data:\")\n    print(f\"  Total samples: {len(texts)}\")\n    print(f\"  Human samples (label=0): {labels.count(0)}\")\n    print(f\"  AI samples (label=1): {labels.count(1)}\")\nelse:\n    print(\"‚ùå No dataset loaded. Cannot continue.\")\n    texts, labels = [], []"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced dataset for Twitter-like short texts\n",
    "def filter_for_twitter(texts, labels, max_chars=500, min_chars=20):\n",
    "    \"\"\"\n",
    "    Filter texts to be more Twitter-like (shorter texts).\n",
    "    Also truncate longer texts to simulate tweet-length content.\n",
    "    \"\"\"\n",
    "    filtered_texts = []\n",
    "    filtered_labels = []\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        # Clean text\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Skip very short texts\n",
    "        if len(text) < min_chars:\n",
    "            continue\n",
    "        \n",
    "        # For longer texts, take first portion (like a tweet thread opener)\n",
    "        if len(text) > max_chars:\n",
    "            # Find a good breaking point\n",
    "            text = text[:max_chars]\n",
    "            last_period = text.rfind('.')\n",
    "            if last_period > max_chars // 2:\n",
    "                text = text[:last_period + 1]\n",
    "        \n",
    "        filtered_texts.append(text)\n",
    "        filtered_labels.append(label)\n",
    "    \n",
    "    return filtered_texts, filtered_labels\n",
    "\n",
    "# Filter for Twitter-like content\n",
    "texts, labels = filter_for_twitter(texts, labels)\n",
    "print(f\"\\nAfter Twitter-like filtering:\")\n",
    "print(f\"  Total samples: {len(texts)}\")\n",
    "print(f\"  Human samples: {labels.count(0)}\")\n",
    "print(f\"  AI samples: {labels.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def balance_dataset(texts, labels, max_per_class=15000):\n",
    "    \"\"\"\n",
    "    Balance dataset to have equal human and AI samples.\n",
    "    \"\"\"\n",
    "    human_texts = [(t, l) for t, l in zip(texts, labels) if l == 0]\n",
    "    ai_texts = [(t, l) for t, l in zip(texts, labels) if l == 1]\n",
    "    \n",
    "    # Shuffle\n",
    "    random.seed(42)\n",
    "    random.shuffle(human_texts)\n",
    "    random.shuffle(ai_texts)\n",
    "    \n",
    "    # Balance\n",
    "    min_samples = min(len(human_texts), len(ai_texts), max_per_class)\n",
    "    \n",
    "    balanced = human_texts[:min_samples] + ai_texts[:min_samples]\n",
    "    random.shuffle(balanced)\n",
    "    \n",
    "    return [t for t, l in balanced], [l for t, l in balanced]\n",
    "\n",
    "texts, labels = balance_dataset(texts, labels)\n",
    "print(f\"\\nBalanced dataset:\")\n",
    "print(f\"  Total samples: {len(texts)}\")\n",
    "print(f\"  Human samples: {labels.count(0)}\")\n",
    "print(f\"  AI samples: {labels.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Second split: 50% val, 50% test from temp\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Train: {len(train_texts)} samples\")\n",
    "print(f\"  Validation: {len(val_texts)} samples\")\n",
    "print(f\"  Test: {len(test_texts)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization\n",
    "\n",
    "Load DistilBERT tokenizer and prepare data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Tokenizer loaded: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': train_texts,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'text': val_texts,\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': test_texts,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128  # Shorter for Twitter-like content\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(\"‚úÖ Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Setup\n",
    "\n",
    "Load DistilBERT and configure for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"human\", 1: \"ai\"},\n",
    "    label2id={\"human\": 0, \"ai\": 1}\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Estimated model size: {total_params * 4 / 1e6:.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary'\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision on GPU\n",
    "    dataloader_num_workers=2,\n",
    "    report_to='none'  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Mixed precision (FP16): {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model\n",
    "\n",
    "‚è±Ô∏è This will take approximately 2-4 hours on a free Colab GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"This may take 2-4 hours on a free GPU.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime'] / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {test_results['eval_accuracy']*100:.2f}%\")\n",
    "print(f\"F1 Score:  {test_results['eval_f1']*100:.2f}%\")\n",
    "print(f\"Precision: {test_results['eval_precision']*100:.2f}%\")\n",
    "print(f\"Recall:    {test_results['eval_recall']*100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "cm = confusion_matrix(true_labels, preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Human', 'AI'],\n",
    "            yticklabels=['Human', 'AI'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix Analysis:\")\n",
    "print(f\"  True Negatives (Human‚ÜíHuman): {cm[0,0]}\")\n",
    "print(f\"  False Positives (Human‚ÜíAI): {cm[0,1]}\")\n",
    "print(f\"  False Negatives (AI‚ÜíHuman): {cm[1,0]}\")\n",
    "print(f\"  True Positives (AI‚ÜíAI): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test with Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "    \"\"\"Predict if text is human or AI generated.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    human_prob = probs[0][0].item()\n",
    "    ai_prob = probs[0][1].item()\n",
    "    \n",
    "    return {\n",
    "        'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "        'human_prob': human_prob,\n",
    "        'ai_prob': ai_prob,\n",
    "        'prediction': 'AI' if ai_prob > 0.5 else 'Human',\n",
    "        'confidence': max(human_prob, ai_prob)\n",
    "    }\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    \"Just had the best coffee at that new place downtown! Highly recommend their oat milk latte ü•§\",\n",
    "    \"The implementation of machine learning algorithms in modern applications has revolutionized the way we process and analyze data, enabling unprecedented levels of efficiency and accuracy.\",\n",
    "    \"lol cant believe what happened today üòÇ my dog literally ate my homework no joke\",\n",
    "    \"In conclusion, the systematic analysis of the aforementioned factors demonstrates a clear correlation between the variables, suggesting that further research is warranted to fully understand the implications.\",\n",
    "    \"anyone else tired of these AI takes? like just let people enjoy things\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sample in test_samples:\n",
    "    result = predict_text(sample)\n",
    "    print(f\"\\nText: {result['text']}\")\n",
    "    print(f\"Prediction: {result['prediction']} ({result['confidence']*100:.1f}% confidence)\")\n",
    "    print(f\"Scores: Human={result['human_prob']*100:.1f}%, AI={result['ai_prob']*100:.1f}%\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = './tweetguard_model'\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {save_path}\")\n",
    "\n",
    "# Check size\n",
    "import os\n",
    "total_size = 0\n",
    "for f in os.listdir(save_path):\n",
    "    size = os.path.getsize(os.path.join(save_path, f))\n",
    "    total_size += size\n",
    "    print(f\"  {f}: {size/1e6:.2f} MB\")\n",
    "print(f\"\\nTotal size: {total_size/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Convert to TensorFlow.js\n",
    "\n",
    "Convert the model for browser deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert to TensorFlow SavedModel format\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "print(\"Converting to TensorFlow format...\")\n",
    "\n",
    "# Load the PyTorch model into TensorFlow\n",
    "tf_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    save_path,\n",
    "    from_pt=True\n",
    ")\n",
    "\n",
    "# Save as TensorFlow SavedModel\n",
    "tf_save_path = './tweetguard_tf_model'\n",
    "tf_model.save_pretrained(tf_save_path, saved_model=True)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow model saved to {tf_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow.js\n",
    "import subprocess\n",
    "\n",
    "tfjs_output_path = './tweetguard_tfjs_model'\n",
    "\n",
    "# Find the saved_model directory\n",
    "saved_model_path = f\"{tf_save_path}/saved_model/1\"\n",
    "\n",
    "print(\"Converting to TensorFlow.js format...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Convert with quantization for smaller size\n",
    "result = subprocess.run([\n",
    "    'tensorflowjs_converter',\n",
    "    '--input_format=tf_saved_model',\n",
    "    '--output_format=tfjs_graph_model',\n",
    "    '--quantize_uint8',  # Quantize to reduce size\n",
    "    '--skip_op_check',\n",
    "    saved_model_path,\n",
    "    tfjs_output_path\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ TensorFlow.js conversion successful!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Conversion output: {result.stdout}\")\n",
    "    print(f\"‚ö†Ô∏è Conversion errors: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TensorFlow.js model size\n",
    "import os\n",
    "\n",
    "if os.path.exists(tfjs_output_path):\n",
    "    total_size = 0\n",
    "    print(f\"\\nTensorFlow.js model files:\")\n",
    "    for f in os.listdir(tfjs_output_path):\n",
    "        filepath = os.path.join(tfjs_output_path, f)\n",
    "        size = os.path.getsize(filepath)\n",
    "        total_size += size\n",
    "        print(f\"  {f}: {size/1e6:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Total TensorFlow.js model size: {total_size/1e6:.2f} MB\")\n",
    "    \n",
    "    if total_size/1e6 < 50:\n",
    "        print(\"‚úÖ Model is suitable for browser deployment!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Model may be too large for optimal browser performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Export Tokenizer for JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Export vocab for JavaScript tokenizer\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Save vocab\n",
    "vocab_path = f\"{tfjs_output_path}/vocab.json\"\n",
    "with open(vocab_path, 'w') as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "print(f\"‚úÖ Vocabulary exported to {vocab_path}\")\n",
    "print(f\"   Vocabulary size: {len(vocab)} tokens\")\n",
    "\n",
    "# Export tokenizer config\n",
    "tokenizer_config = {\n",
    "    'max_length': 128,\n",
    "    'pad_token_id': tokenizer.pad_token_id,\n",
    "    'cls_token_id': tokenizer.cls_token_id,\n",
    "    'sep_token_id': tokenizer.sep_token_id,\n",
    "    'unk_token_id': tokenizer.unk_token_id,\n",
    "    'do_lower_case': True\n",
    "}\n",
    "\n",
    "config_path = f\"{tfjs_output_path}/tokenizer_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(tokenizer_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Tokenizer config exported to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Download Model Files\n",
    "\n",
    "Download the TensorFlow.js model files to use in the Chrome extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model for download\n",
    "import shutil\n",
    "\n",
    "zip_path = './tweetguard_tfjs_model.zip'\n",
    "shutil.make_archive(\n",
    "    zip_path.replace('.zip', ''),\n",
    "    'zip',\n",
    "    tfjs_output_path\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model zipped to {zip_path}\")\n",
    "\n",
    "# Download link (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\nüì• Downloading model...\")\n",
    "    files.download(zip_path)\n",
    "except:\n",
    "    print(f\"\\nüìÅ Model ready at: {zip_path}\")\n",
    "    print(\"Download it manually or use the Files panel on the left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download** the `tweetguard_tfjs_model.zip` file\n",
    "2. **Extract** it to your Chrome extension's `model/` directory\n",
    "3. **Update** your extension's `detector.js` to load the TensorFlow.js model\n",
    "\n",
    "### Model Files You'll Need:\n",
    "- `model.json` - Model architecture\n",
    "- `group*.bin` - Model weights (sharded)\n",
    "- `vocab.json` - Tokenizer vocabulary\n",
    "- `tokenizer_config.json` - Tokenizer settings\n",
    "\n",
    "### Expected Performance:\n",
    "- **Accuracy**: ~75-82%\n",
    "- **Model Size**: ~15-50MB (with quantization)\n",
    "- **Inference Time**: ~10-30ms per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: DistilBERT-base-uncased\")\n",
    "print(f\"Task: Binary Classification (Human vs AI)\")\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']*100:.2f}%\")\n",
    "print(f\"Test F1 Score: {test_results['eval_f1']*100:.2f}%\")\n",
    "print(f\"\")\n",
    "print(f\"Output files:\")\n",
    "print(f\"  - PyTorch model: {save_path}\")\n",
    "print(f\"  - TensorFlow model: {tf_save_path}\")\n",
    "print(f\"  - TensorFlow.js model: {tfjs_output_path}\")\n",
    "print(f\"  - Download: {zip_path}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ Ready for Chrome extension integration!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}